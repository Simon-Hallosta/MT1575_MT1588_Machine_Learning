{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd5ef9a",
   "metadata": {},
   "source": [
    "# Lab — Logistic Regression \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983572c",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "Run the next cell first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242dae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3052fffc",
   "metadata": {},
   "source": [
    "## 1) Binary classification + simulated data \n",
    "\n",
    "In the lecture:\n",
    "- labels are **0** (negative class) and **1** (positive class)\n",
    "- we want to estimate the probability $P(y=1\\mid x)$\n",
    "\n",
    "We simulate a 2D dataset so we can plot:\n",
    "- the points\n",
    "- the decision boundary\n",
    "- how the model changes during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e00d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples=400,\n",
    "    centers=2,\n",
    "    n_features=2,\n",
    "    cluster_std=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features (fit only on training data)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s  = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_data(X, y, title=\"\"):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], alpha=0.7, label=\"Class 0\")\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], alpha=0.7, label=\"Class 1\")\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_data(X_train_s, y_train, title=\"Training data (standardized)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201232ba",
   "metadata": {},
   "source": [
    "## 2) Model: hypothesis + sigmoid\n",
    "\n",
    "### Linear score\n",
    "We first compute a linear combination:\n",
    "\n",
    "$$\n",
    "z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2\n",
    "$$\n",
    "\n",
    "### Logistic hypothesis\n",
    "Then we pass it through the sigmoid function:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "### Probabilistic interpretation\n",
    "We interpret:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = P(y=1\\mid x)\n",
    "$$\n",
    "\n",
    "and therefore:\n",
    "\n",
    "$$\n",
    "P(y=0\\mid x) = 1 - h_\\theta(x)\n",
    "$$\n",
    "\n",
    "### Useful calculus fact \n",
    "Derivative of sigmoid:\n",
    "\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z)(1-\\sigma(z))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d50cd",
   "metadata": {},
   "source": [
    "## 3) Cost function (Log Loss / Negative Log-Likelihood)\n",
    "\n",
    "\n",
    "\n",
    "For a single sample:\n",
    "- If $y=1$: cost is $-\\log(h_\\theta(x))$\n",
    "- If $y=0$: cost is $-\\log(1-h_\\theta(x))$\n",
    "\n",
    "Combined into one expression:\n",
    "\n",
    "$$\n",
    "\\text{cost}(h_\\theta(x), y) = -\\left[y\\log(h_\\theta(x)) + (1-y)\\log(1-h_\\theta(x))\\right]\n",
    "$$\n",
    "\n",
    "For the dataset (mean cost):\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m} \\text{cost}(h_\\theta(x^{(i)}), y^{(i)})\n",
    "$$\n",
    "\n",
    "We'll implement this **as log loss** in code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a179a2",
   "metadata": {},
   "source": [
    "### 3.1) TODO — implement the key building blocks\n",
    "\n",
    "Fill in:\n",
    "- `sigmoid(z)`\n",
    "- `predict_proba(X, theta)`  (returns $h_\\theta(x)$ for each row)\n",
    "- `log_loss_cost(y, p)`      (mean log loss / NLL)\n",
    "\n",
    "**(For extra numerical stability):**\n",
    "Use `np.clip(p, eps, 1-eps)` to avoid `log(0)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1758904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# TODO (STUDENTS)\n",
    "# ============================================================\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"sigma(z) = 1/(1+exp(-z))\"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "def predict_proba(X, theta):\n",
    "    \"\"\"Return h_theta(x) for each row in X.\n",
    "    Convention: theta[0] is the bias term (intercept).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "def log_loss_cost(y_true, p_pred, eps=1e-12):\n",
    "    \"\"\"Mean log loss / negative log-likelihood.\"\"\"\n",
    "    # TODO\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b088c",
   "metadata": {},
   "source": [
    "## 4) Gradient Descent \n",
    "\n",
    "In the lecture, the update was shown as:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\cdot \\frac{1}{m}\\sum_{i=1}^{m}\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)}\n",
    "$$\n",
    "\n",
    "and the intercept term (bias) updates similarly (with $x_0^{(i)}=1$).\n",
    "\n",
    "### TODO\n",
    "Implement the gradient step using the formula above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b50369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# TODO (STUDENTS)\n",
    "# ============================================================\n",
    "\n",
    "def fit_logreg_gd(X, y, lr=0.2, n_steps=250):\n",
    "    \"\"\"Gradient descent for logistic regression.\n",
    "    theta[0] is bias, theta[1:] are feature weights.\n",
    "    \"\"\"\n",
    "    m, d = X.shape\n",
    "    theta = np.zeros(d + 1)\n",
    "    losses = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        p = predict_proba(X, theta)\n",
    "        losses.append(log_loss_cost(y, p))\n",
    "\n",
    "        # TODO: compute gradients according to the slide formula\n",
    "        # For j=0 (bias): use x0 = 1\n",
    "        # For j>=1: use the feature column\n",
    "        pass\n",
    "\n",
    "    return theta, np.array(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168fa0f",
   "metadata": {},
   "source": [
    "## 5) Train + visualize learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta, losses = fit_logreg_gd(X_train_s, y_train, lr=0.2, n_steps=250)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Gradient descent step\")\n",
    "plt.ylabel(\"Log loss / NLL\")\n",
    "plt.title(\"Training cost J(θ) (should decrease)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea9811",
   "metadata": {},
   "source": [
    "## 6) Decision boundary \n",
    "\n",
    "Recall the logistic-regression prediction rule with threshold 0.5:\n",
    "\n",
    "$$\n",
    "\\hat{y}=1 \\quad \\text{if} \\quad h_\\theta(x) \\ge 0.5\n",
    "$$\n",
    "\n",
    "Because the sigmoid satisfies $\\sigma(z)=0.5$ exactly when $z=0$, the **decision boundary** is the set of points where the linear score is zero:\n",
    "\n",
    "$$\n",
    "z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = 0\n",
    "$$\n",
    "\n",
    "That equation describes a **line** in the $(x_1, x_2)$ plane. To plot it, we typically solve for $x_2$ as a function of $x_1$:\n",
    "\n",
    "$$\n",
    "\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = 0\n",
    "\\;\\;\\Rightarrow\\;\\;\n",
    "x_2 = -\\frac{\\theta_0 + \\theta_1 x_1}{\\theta_2}\n",
    "$$\n",
    "\n",
    "### Your task\n",
    "Implement a function:\n",
    "\n",
    "- **Name:** `decision_boundary_y(x1, theta)`\n",
    "- **Input:** \n",
    "  - `x1`: a NumPy array of $x_1$ values\n",
    "  - `theta`: a length-3 vector $[\\theta_0,\\theta_1,\\theta_2]$\n",
    "- **Output:** \n",
    "  - a NumPy array with the corresponding boundary values $x_2$ computed from the formula above.\n",
    "\n",
    "⚠️ Edge case: if $\\theta_2 \\approx 0$, the boundary is (nearly) a **vertical line** and the formula above would divide by zero.  \n",
    "In that case, return something safe (e.g. `np.zeros_like(x1)`) so the code runs, and leave a short comment explaining why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f2933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the decision boundary function based on the derivation above.\n",
    "def decision_boundary_y(x1, theta):\n",
    "    \"\"\"\n",
    "    Return x2 values for the decision boundary z=0:\n",
    "        theta0 + theta1*x1 + theta2*x2 = 0  ->  x2 = -(theta0 + theta1*x1)/theta2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x1 : np.ndarray\n",
    "        Array of x1 values (already in the same feature space as training data, e.g. scaled).\n",
    "    theta : array-like, shape (3,)\n",
    "        [theta0, theta1, theta2]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x2 : np.ndarray\n",
    "        Boundary x2 values for each x1.\n",
    "    \"\"\"\n",
    "    # --- STUDENT TODO START ---\n",
    "    raise NotImplementedError(\"Implement decision_boundary_y(x1, theta)\")\n",
    "    # --- STUDENT TODO END ---\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X_train_s[y_train==0, 0], X_train_s[y_train==0, 1], alpha=0.35, label=\"Class 0\")\n",
    "plt.scatter(X_train_s[y_train==1, 0], X_train_s[y_train==1, 1], alpha=0.35, label=\"Class 1\")\n",
    "\n",
    "x1 = np.linspace(X_train_s[:, 0].min() - 1, X_train_s[:, 0].max() + 1, 200)\n",
    "plt.plot(x1, decision_boundary_y(x1, theta), linewidth=3, label=\"Decision boundary (p=0.5)\")\n",
    "\n",
    "plt.title(\"Decision boundary (from scratch)\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecf554",
   "metadata": {},
   "source": [
    "## 8) Evaluate on test set (accuracy + confusion matrix + probability histogram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_test = predict_proba(X_test_s, theta)\n",
    "y_pred = (p_test >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test accuracy:\", acc)\n",
    "print(\"Test log loss:\", log_loss(y_test, p_test))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.title(f\"Confusion Matrix (from scratch) — acc={acc:.3f}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(p_test[y_test==0], bins=20, alpha=0.7, label=\"True class 0\")\n",
    "plt.hist(p_test[y_test==1], bins=20, alpha=0.7, label=\"True class 1\")\n",
    "plt.xlabel(\"Predicted probability $h_\\theta(x)$\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Predicted probabilities on test set\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5750f",
   "metadata": {},
   "source": [
    "## 9) Regularization (L2 and L1)\n",
    "\n",
    "To reduce overfitting, we modify the objective function by adding a penalty term\n",
    "that discourages large parameter values.\n",
    "\n",
    "Recall the gradient descent update rule:\n",
    "\n",
    "$$\n",
    "\\theta_j :=\n",
    "\\theta_j\n",
    "-\n",
    "\\alpha\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### L2 regularization (Ridge)\n",
    "\n",
    "We define the regularized objective:\n",
    "\n",
    "$$\n",
    "J_{reg}(\\theta)\n",
    "=\n",
    "J(\\theta)\n",
    "+\n",
    "\\lambda\n",
    "\\sum_{j=1}^{n} \\theta_j^2\n",
    "$$\n",
    "\n",
    "The update rule becomes:\n",
    "\n",
    "$$\n",
    "\\theta_j :=\n",
    "\\theta_j\n",
    "-\n",
    "\\alpha\n",
    "\\left(\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "+\n",
    "2\\lambda\\theta_j\n",
    "\\right),\n",
    "\\quad j \\ge 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### L1 regularization (Lasso)\n",
    "\n",
    "$$\n",
    "J_{reg}(\\theta)\n",
    "=\n",
    "J(\\theta)\n",
    "+\n",
    "\\lambda\n",
    "\\sum_{j=1}^{n} |\\theta_j|\n",
    "$$\n",
    "\n",
    "Using a sub-gradient:\n",
    "\n",
    "$$\n",
    "\\theta_j :=\n",
    "\\theta_j\n",
    "-\n",
    "\\alpha\n",
    "\\left(\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "+\n",
    "\\lambda\\,\\text{sign}(\\theta_j)\n",
    "\\right),\n",
    "\\quad j \\ge 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "> The bias term $\\theta_0$ is typically **not regularized**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795fcc51",
   "metadata": {},
   "source": [
    "### Dataset where regularization actually helps\n",
    "\n",
    "We keep the 2 real features (so the boundary is plottable), but we add many **noise features**.\n",
    "Without regularization, the model can overfit by giving large weights to noise features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639642da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a \"harder\" version: 2 real features + many noise features\n",
    "X2, y2 = make_blobs(\n",
    "    n_samples=500, centers=2, n_features=2, cluster_std=2.2, random_state=7\n",
    ")\n",
    "n_noise = 20\n",
    "noise = np.random.normal(size=(X2.shape[0], n_noise))\n",
    "Xn = np.hstack([X2, noise])\n",
    "\n",
    "Xn_train, Xn_test, yn_train, yn_test = train_test_split(\n",
    "    Xn, y2, test_size=0.3, random_state=42, stratify=y2\n",
    ")\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "Xn_train_s = scaler2.fit_transform(Xn_train)\n",
    "Xn_test_s  = scaler2.transform(Xn_test)\n",
    "\n",
    "plot_data(Xn_train_s[:, :2], yn_train, title=\"Training data (first two features) + hidden noise features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc65f8",
   "metadata": {},
   "source": [
    "### TODO — implement regularized gradient descent\n",
    "\n",
    "Keep the same structure as before, but add the extra gradient term:\n",
    "- `reg=None`\n",
    "- `reg=\"l2\"`\n",
    "- `reg=\"l1\"`\n",
    "\n",
    "Remember: **do not regularize** the bias term `theta[0]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78167a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# TODO: Implement gradient descent with regularization (L2 and L1) \n",
    "# ============================================================\n",
    "\n",
    "def fit_logreg_gd_regularized(X, y, lr=0.2, n_steps=250, reg=None, lam=0.1):\n",
    "    m, d = X.shape\n",
    "    theta = np.zeros(d + 1)\n",
    "    losses = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        p = predict_proba(X, theta)\n",
    "        losses.append(log_loss_cost(y, p))\n",
    "\n",
    "        diff = (p - y)\n",
    "        grad0 = np.mean(diff)\n",
    "        grad_rest = (X.T @ diff) / m\n",
    "\n",
    "        # =======================================================\n",
    "        # TODO: add regularization \n",
    "        # =======================================================\n",
    "\n",
    "\n",
    "    return theta, np.array(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aba185",
   "metadata": {},
   "source": [
    "### Evaluate and compare regularization \n",
    "\n",
    "We will compare **No regularization vs L2 vs L1** using:\n",
    "- **Accuracy** (based on hard predictions $\\hat{y}$)\n",
    "- **Log loss** (based on probabilities $p$)\n",
    "\n",
    "#### Your task (mandatory)\n",
    "In the function `eval_theta(Xt, yt, theta)` you must compute:\n",
    "\n",
    "1) **Predicted probabilities**  \n",
    "\n",
    "$$\n",
    "p = h_\\theta(x) \\in [0,1]\n",
    "$$\n",
    "\n",
    "2) **Hard class predictions**  \n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } p \\ge 0.5 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So in code:\n",
    "- `p` must be a NumPy array of probabilities\n",
    "- `yhat` must be a NumPy array of integers (0/1)\n",
    "\n",
    "Only after you compute `p` and `yhat` should you return:\n",
    "- `accuracy_score(yt, yhat)`\n",
    "- `log_loss(yt, p)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta_none, loss_none = fit_logreg_gd_regularized(Xn_train_s, yn_train, reg=None, lam=0.1, lr=0.2, n_steps=300)\n",
    "theta_l2,   loss_l2   = fit_logreg_gd_regularized(Xn_train_s, yn_train, reg=\"l2\", lam=0.1, lr=0.2, n_steps=300)\n",
    "theta_l1,   loss_l1   = fit_logreg_gd_regularized(Xn_train_s, yn_train, reg=\"l1\", lam=0.05, lr=0.2, n_steps=300)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TODO (STUDENTS): implement p and yhat\n",
    "# ============================================================\n",
    "def eval_theta(Xt, yt, theta):\n",
    "    \"\"\"\n",
    "    Evaluate a trained parameter vector theta on a test set.\n",
    "\n",
    "    YOU must implement:\n",
    "      - p: predicted probabilities\n",
    "      - yhat: predicted labels using threshold 0.5\n",
    "\n",
    "    Returns:\n",
    "        - accuracy score\n",
    "        - log loss (NLL)\n",
    "    \"\"\"\n",
    "    # --- STUDENT TODO START ---\n",
    "    # 1) p = ...\n",
    "    # 2) yhat = ...\n",
    "    # return accuracy_score(yt, yhat), log_loss(yt, p)\n",
    "    raise NotImplementedError(\"Compute p and yhat in eval_theta()\")\n",
    "    # --- STUDENT TODO END ---\n",
    "\n",
    "\n",
    "\n",
    "acc_none, ll_none = eval_theta(Xn_test_s, yn_test, theta_none)\n",
    "acc_l2,   ll_l2   = eval_theta(Xn_test_s, yn_test, theta_l2)\n",
    "acc_l1,   ll_l1   = eval_theta(Xn_test_s, yn_test, theta_l1)\n",
    "\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "#----------- (1) cost curves----------\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1.plot(loss_none, label=f\"No reg (acc={acc_none:.3f})\")\n",
    "ax1.plot(loss_l2,   label=f\"L2 (acc={acc_l2:.3f})\")\n",
    "ax1.plot(loss_l1,   label=f\"L1 (acc={acc_l1:.3f})\")\n",
    "ax1.set_title(\"Training cost J(θ)\")\n",
    "ax1.set_xlabel(\"GD step\")\n",
    "ax1.set_ylabel(\"Log loss / NLL\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "#---------- (2) boundaries (first two features)----------\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "\n",
    "X2_train = Xn_train_s[:, :2]\n",
    "ax2.scatter(X2_train[yn_train==0,0], X2_train[yn_train==0,1], alpha=0.35, label=\"Class 0\")\n",
    "ax2.scatter(X2_train[yn_train==1,0], X2_train[yn_train==1,1], alpha=0.35, label=\"Class 1\")\n",
    "\n",
    "# Compute axis limits from the data (with padding)\n",
    "x_min, x_max = X2_train[:,0].min(), X2_train[:,0].max()\n",
    "y_min, y_max = X2_train[:,1].min(), X2_train[:,1].max()\n",
    "pad_x = 0.10 * (x_max - x_min + 1e-12)\n",
    "pad_y = 0.10 * (y_max - y_min + 1e-12)\n",
    "\n",
    "ax2.set_xlim(x_min - pad_x, x_max + pad_x)\n",
    "ax2.set_ylim(y_min - pad_y, y_max + pad_y)\n",
    "\n",
    "x1 = np.linspace(x_min - pad_x, x_max + pad_x, 300)\n",
    "\n",
    "def plot_boundary(ax, theta, label, **kwargs):\n",
    "    # If theta2 is ~0 -> boundary is (approximately) vertical: theta0 + theta1*x1 = 0\n",
    "    if abs(theta[2]) < 1e-6:\n",
    "        if abs(theta[1]) < 1e-12:\n",
    "            return  # degenerate\n",
    "        x1_star = -theta[0] / theta[1]\n",
    "        ax.axvline(x1_star, label=label, **kwargs)\n",
    "        return\n",
    "\n",
    "    x2 = -(theta[0] + theta[1]*x1) / theta[2]\n",
    "\n",
    "    # Clip the line to the visible y-range so it can't blow up the plot\n",
    "    y_lo, y_hi = ax.get_ylim()\n",
    "    x2_clipped = np.clip(x2, y_lo, y_hi)\n",
    "\n",
    "    ax.plot(x1, x2_clipped, label=label, **kwargs)\n",
    "\n",
    "plot_boundary(ax2, theta_none, \"No reg\", linewidth=3)\n",
    "plot_boundary(ax2, theta_l2,   \"L2\", linestyle=\"--\", linewidth=2)\n",
    "plot_boundary(ax2, theta_l1,   \"L1\", linestyle=\":\", linewidth=3)\n",
    "\n",
    "ax2.set_title(\"Decision boundary (first 2 features)\")\n",
    "ax2.set_xlabel(\"$x_1$\")\n",
    "ax2.set_ylabel(\"$x_2$\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "#------------- (3) weight magnitudes (excluding bias)--------------------\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3.hist(np.abs(theta_none[1:]), bins=20, alpha=0.6, label=\"No reg\")\n",
    "ax3.hist(np.abs(theta_l2[1:]), bins=20, alpha=0.6, label=\"L2\")\n",
    "ax3.hist(np.abs(theta_l1[1:]), bins=20, alpha=0.6, label=\"L1\")\n",
    "ax3.set_title(\"Distribution of |weights| (excluding bias)\")\n",
    "ax3.set_xlabel(\"|θ_j|\")\n",
    "ax3.set_ylabel(\"Count\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Test metrics (noise-features dataset):\")\n",
    "print(f\"No reg: acc={acc_none:.3f}, log-loss={ll_none:.3f}\")\n",
    "print(f\"L2    : acc={acc_l2:.3f}, log-loss={ll_l2:.3f}\")\n",
    "print(f\"L1    : acc={acc_l1:.3f}, log-loss={ll_l1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c85156",
   "metadata": {},
   "source": [
    "## 10) Sanity-check with sklearn\n",
    "\n",
    "Sklearn is used only as a check that the boundary is consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Train sklearn on SAME dataset as our from-scratch model\n",
    "# ------------------------------------------------------------\n",
    "clf = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", max_iter=2000)\n",
    "clf.fit(Xn_train_s, yn_train)\n",
    "theta_sk = np.r_[clf.intercept_.item(), clf.coef_.ravel()]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute test metrics (no plotting here)\n",
    "# ------------------------------------------------------------\n",
    "# From scratch (L2 model)\n",
    "p_scratch = predict_proba(Xn_test_s, theta_l2)\n",
    "yhat_scratch = (p_scratch >= 0.5).astype(int)\n",
    "acc_scratch = accuracy_score(yn_test, yhat_scratch)\n",
    "ll_scratch = log_loss(yn_test, p_scratch)\n",
    "\n",
    "# sklearn\n",
    "p_sk = clf.predict_proba(Xn_test_s)[:, 1]\n",
    "yhat_sk = (p_sk >= 0.5).astype(int)\n",
    "acc_sk = accuracy_score(yn_test, yhat_sk)\n",
    "ll_sk = log_loss(yn_test, p_sk)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot: decision boundary slice only\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "X2_train = Xn_train_s[:, :2]\n",
    "plt.scatter(X2_train[yn_train == 0, 0], X2_train[yn_train == 0, 1],\n",
    "            alpha=0.35, label=\"Class 0\")\n",
    "plt.scatter(X2_train[yn_train == 1, 0], X2_train[yn_train == 1, 1],\n",
    "            alpha=0.35, label=\"Class 1\")\n",
    "\n",
    "# axis limits from data (with padding)\n",
    "x_min, x_max = X2_train[:, 0].min(), X2_train[:, 0].max()\n",
    "y_min, y_max = X2_train[:, 1].min(), X2_train[:, 1].max()\n",
    "pad_x = 0.10 * (x_max - x_min + 1e-12)\n",
    "pad_y = 0.10 * (y_max - y_min + 1e-12)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(x_min - pad_x, x_max + pad_x)\n",
    "ax.set_ylim(y_min - pad_y, y_max + pad_y)\n",
    "\n",
    "x1_grid = np.linspace(x_min - pad_x, x_max + pad_x, 400)\n",
    "\n",
    "def plot_boundary_slice(ax, theta, x1_grid, label, x_rest=None, **kwargs):\n",
    "    d = len(theta) - 1\n",
    "    if x_rest is None:\n",
    "        x_rest = np.zeros(max(d - 2, 0))\n",
    "\n",
    "    # Near-vertical boundary\n",
    "    if abs(theta[2]) < 1e-8:\n",
    "        denom = theta[1]\n",
    "        if abs(denom) < 1e-12:\n",
    "            return\n",
    "        const = theta[0]\n",
    "        if d > 2:\n",
    "            const += np.dot(theta[3:], x_rest)\n",
    "        x1_star = -const / denom\n",
    "        ax.axvline(x1_star, label=label, **kwargs)\n",
    "        return\n",
    "\n",
    "    const = theta[0] + theta[1] * x1_grid\n",
    "    if d > 2:\n",
    "        const = const + np.dot(theta[3:], x_rest)\n",
    "\n",
    "    x2 = -const / theta[2]\n",
    "\n",
    "    # Mask outside current y-limits (no clipping artefacts)\n",
    "    y_lo, y_hi = ax.get_ylim()\n",
    "    x2_masked = np.where((x2 >= y_lo) & (x2 <= y_hi), x2, np.nan)\n",
    "\n",
    "    ax.plot(x1_grid, x2_masked, label=label, **kwargs)\n",
    "\n",
    "plot_boundary_slice(ax, theta_l2, x1_grid,\n",
    "                    \"From scratch (L2 slice)\", linewidth=3)\n",
    "plot_boundary_slice(ax, theta_sk, x1_grid,\n",
    "                    \"sklearn (L2 slice)\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "plt.title(\"Decision boundary comparison (2D slice)\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Print comparison results clearly\n",
    "# ------------------------------------------------------------\n",
    "print(\"Test metrics on SAME noise-features dataset\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(f\"From scratch (L2):\")\n",
    "print(f\"  Accuracy : {acc_scratch:.4f}\")\n",
    "print(f\"  Log-loss : {ll_scratch:.4f}\")\n",
    "print()\n",
    "print(f\"sklearn LogisticRegression (L2):\")\n",
    "print(f\"  Accuracy : {acc_sk:.4f}\")\n",
    "print(f\"  Log-loss : {ll_sk:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matmod-2022-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
