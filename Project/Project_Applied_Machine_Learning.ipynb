{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc61ff90",
   "metadata": {},
   "source": [
    "# Applied Machine Learning — Course Project \n",
    "## Corrupted Occupancy Dataset: Cleaning → Unsupervised → Supervised → Model Selection\n",
    "\n",
    "**Course:** MT1575 Applied Machine Learning  \n",
    "**Deliverable:** Submit this notebook (`.ipynb`) with all outputs visible (plots, tables, printed metrics).\n",
    "\n",
    "---\n",
    "\n",
    "## What we assess\n",
    "Your grade is primarily based on:\n",
    "\n",
    "1. **A correct ML workflow** (cleaning → preprocessing → modelling → evaluation)\n",
    "2. **Model selection quality** (hyperparameters + comparison across models)\n",
    "3. **Overfitting/underfitting analysis** (train vs validation behaviour)\n",
    "4. **Clear communication through visual evidence** (many plots + compact tables)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Tracks\n",
    "Set `TRACK` in the setup cell:\n",
    "- `TRACK=\"3hp\"`: baseline requirements\n",
    "- `TRACK=\"4.5hp\"`: baseline + **Step 10 extension** (required for 4.5 hp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018fb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Setup =====\n",
    "TRACK = \"3hp\"          # \"3hp\" or \"4.5hp\"\n",
    "DATA_FILE = \"MT1575_occupancy_corrupted.csv\"  # upload to notebook working directory\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a7d2c",
   "metadata": {},
   "source": [
    "## 1) Rules: allowed vs required\n",
    "\n",
    "### You MAY use sklearn for\n",
    "- splitting: `train_test_split`, `StratifiedKFold`\n",
    "- preprocessing: `StandardScaler`, `OneHotEncoder`, `ColumnTransformer`\n",
    "- metrics/plots: `accuracy_score`, `f1_score`, `roc_auc_score`, confusion matrix, ROC/PR displays\n",
    "- models: `DecisionTreeClassifier`, `RandomForestClassifier`, `MLPClassifier`\n",
    "\n",
    "### You MUST implement from scratch (functions)\n",
    "You must implement and **use** these yourself:\n",
    "- Logistic regression training with gradient descent (functions)\n",
    "- kNN prediction (functions)\n",
    "- PCA (functions: SVD or eigen-decomposition)\n",
    "- k-means clustering (functions)\n",
    "\n",
    "### You may NOT use (for the from-scratch parts)\n",
    "- `sklearn.linear_model.LogisticRegression`\n",
    "- `sklearn.neighbors.KNeighborsClassifier`\n",
    "- `sklearn.decomposition.PCA`\n",
    "- `sklearn.cluster.KMeans`\n",
    "\n",
    "If forbidden classes are used for from-scratch parts, that part is **not submitted**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e65aa31",
   "metadata": {},
   "source": [
    "## 2) Load dataset\n",
    "\n",
    "### What we expect to see (outputs)\n",
    "- Dataset shape printed\n",
    "- First 5 rows displayed\n",
    "- Column list printed\n",
    "- Dtypes displayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b263af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(DATA_FILE)\n",
    "assert path.exists(), f\"Could not find {DATA_FILE}. Upload it first.\"\n",
    "\n",
    "df_raw = pd.read_csv(path)\n",
    "\n",
    "display(df_raw.head())\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "print(\"Columns:\", list(df_raw.columns))\n",
    "display(df_raw.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b28d37d",
   "metadata": {},
   "source": [
    "## 3) Data cleaning (YOU implement)\n",
    "\n",
    "The dataset is intentionally corrupted (types, missing values, strange strings, outliers, duplicates).\n",
    "\n",
    "### Hard requirements (we will check these outputs)\n",
    "#### A) Before/after tables\n",
    "1. Missing values table **before** and **after** cleaning  \n",
    "   - show at least the top 10 columns with most missing values\n",
    "2. Dtype table **before** and **after** cleaning\n",
    "\n",
    "#### B) Plots (minimum 4)\n",
    "You must show at least:\n",
    "1. One plot showing a data problem **before** cleaning\n",
    "2. The same (or equivalent) plot **after** cleaning\n",
    "3. One histogram (before/after ideal)\n",
    "4. One target-label plot (bar chart of `Occupancy` values before/after)\n",
    "\n",
    "#### C) Output contract\n",
    "By the end of this step, you must have:\n",
    "- `df` (cleaned dataframe)\n",
    "- `TARGET_COL = \"Occupancy\"` exists\n",
    "- `df[TARGET_COL]` is binary integers in `{0,1}`\n",
    "- Features used for modelling contain **no missing values**\n",
    "\n",
    "### What you must write (short text)\n",
    "In a markdown cell below your cleaning code, write **5–10 lines** explaining:\n",
    "- which issues you found,\n",
    "- what you changed,\n",
    "- and why those choices are reasonable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"Occupancy\"\n",
    "df = df_raw.copy()\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: Cleaning pipeline\n",
    "# ----------------------------\n",
    "# Suggested steps:\n",
    "# 1) Standardize column names (strip, lower, underscores)\n",
    "# 2) Replace sentinel strings/values with NaN\n",
    "# 3) Convert numeric columns stored as strings (comma->dot)\n",
    "# 4) Remove duplicates\n",
    "# 5) Handle missing values (drop or impute)\n",
    "# 6) Handle outliers / invalid values (clip/remove)\n",
    "# 7) Fix target labels: ensure exactly {0,1} integers\n",
    "\n",
    "# ----------------------------\n",
    "# REQUIRED: Before/after evidence\n",
    "# ----------------------------\n",
    "# TODO: missing values before/after tables\n",
    "# TODO: dtype before/after table\n",
    "# TODO: minimum 4 plots\n",
    "\n",
    "print(\"After cleaning:\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Target unique:\", sorted(pd.Series(df[TARGET_COL]).dropna().unique()) if TARGET_COL in df.columns else \"TARGET_COL missing\")\n",
    "display(df.head())\n",
    "display(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a5a9f",
   "metadata": {},
   "source": [
    "### Cleaning summary (required: 5–10 lines)\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e752da",
   "metadata": {},
   "source": [
    "## 4) Exploratory Data Analysis (EDA)\n",
    "\n",
    "### Hard requirements (we will check these outputs)\n",
    "#### A) Class balance\n",
    "- A table with counts and percentages for `Occupancy`.\n",
    "\n",
    "#### B) Plots (minimum 6)\n",
    "Include at least:\n",
    "1. Two histograms (two different numeric features)\n",
    "2. Two boxplots (two different numeric features)\n",
    "3. One scatter plot of two features colored by `Occupancy`\n",
    "4. One correlation visualization (heatmap OR top-10 correlations plot)\n",
    "\n",
    "#### C) Short EDA text (required)\n",
    "Write **5–10 lines**:\n",
    "- what patterns you see,\n",
    "- what might cause modelling difficulty,\n",
    "- whether scaling/outliers/class imbalance seems important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1fe63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class balance table\n",
    "\n",
    "# TODO: minimum 6 plots (hist, boxplot, scatter, correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1371f9e",
   "metadata": {},
   "source": [
    "### EDA summary (required: 5–10 lines)\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983457e8",
   "metadata": {},
   "source": [
    "## 5) Train/Validation/Test split + preprocessing (infrastructure cell)\n",
    "\n",
    "This section sets up a standard ML pipeline. You are not graded on writing the split code itself,\n",
    "but you are graded on using the resulting arrays correctly in later steps.\n",
    "\n",
    "### What we expect you to do in the code cell below\n",
    "- (Optional) edit `DROP_COLS` if you identify ID/leakage columns.\n",
    "- Run the cell.\n",
    "- Verify the printed output:\n",
    "  - shapes are correct,\n",
    "  - class balance is similar across splits,\n",
    "  - processed arrays have consistent feature dimensions.\n",
    "\n",
    "### Output (required variables)\n",
    "This cell must produce:\n",
    "- `Xtr_np`, `Xva_np`, `Xte_np`\n",
    "- `y_train`, `y_val`, `y_test`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Optional: drop ID/leakage columns\n",
    "DROP_COLS = []\n",
    "\n",
    "X = df.drop(columns=[TARGET_COL] + DROP_COLS)\n",
    "y = df[TARGET_COL].astype(int)\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train_full\n",
    ")\n",
    "\n",
    "def balance_table(y_):\n",
    "    vc = y_.value_counts()\n",
    "    return pd.DataFrame({\"count\": vc, \"pct\": (vc / vc.sum()).round(3)})\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "display(pd.concat({\n",
    "    \"train\": balance_table(y_train),\n",
    "    \"val\": balance_table(y_val),\n",
    "    \"test\": balance_table(y_test),\n",
    "}, axis=1))\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "Xtr = preprocessor.fit_transform(X_train)\n",
    "Xva = preprocessor.transform(X_val)\n",
    "Xte = preprocessor.transform(X_test)\n",
    "\n",
    "Xtr_np = Xtr.toarray() if hasattr(Xtr, \"toarray\") else np.asarray(Xtr)\n",
    "Xva_np = Xva.toarray() if hasattr(Xva, \"toarray\") else np.asarray(Xva)\n",
    "Xte_np = Xte.toarray() if hasattr(Xte, \"toarray\") else np.asarray(Xte)\n",
    "\n",
    "print(\"Processed shapes:\", Xtr_np.shape, Xva_np.shape, Xte_np.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe9bc1e",
   "metadata": {},
   "source": [
    "## 6) From-scratch functions (required)\n",
    "\n",
    "You must implement and use the functions below in later sections.\n",
    "\n",
    "### Logistic Regression (scratch)\n",
    "You must implement:\n",
    "- `sigmoid(z)` (numerically stable)\n",
    "- `logreg_loss_and_grad(X, y, w, l2)` → returns `(loss, grad)`\n",
    "- `fit_logreg_gd(X, y, lr, n_steps, l2)` → returns `(w, loss_history)`\n",
    "- `predict_proba_logreg(X, w)` and `predict_logreg(X, w, threshold)`\n",
    "\n",
    "**What we expect to see**\n",
    "- A plot of loss vs iteration for at least one run\n",
    "- A complexity curve: train vs validation F1 vs `l2`\n",
    "\n",
    "### kNN (scratch)\n",
    "You must implement:\n",
    "- `knn_predict(X_train, y_train, X_query, k, weighted=False)`\n",
    "\n",
    "**What we expect to see**\n",
    "- A complexity curve: train vs validation F1 vs `k`\n",
    "\n",
    "### PCA (scratch)\n",
    "You must implement:\n",
    "- `pca_fit(X, n_components)` → returns `mean`, `components`, `explained_variance_ratio`\n",
    "- `pca_transform(X, pca_model)`\n",
    "\n",
    "**What we expect to see**\n",
    "- Cumulative explained variance plot\n",
    "- PCA-2D scatter colored by Occupancy\n",
    "\n",
    "### k-means (scratch)\n",
    "You must implement:\n",
    "- `kmeans_fit(X, k, ...)` → returns `centroids`, `labels`, `inertia_history`\n",
    "- `kmeans_predict(X, centroids)`\n",
    "\n",
    "**What we expect to see**\n",
    "- silhouette score vs k plot\n",
    "- PCA-2D scatter colored by clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Logistic Regression (scratch)\n",
    "# -----------------------------\n",
    "def sigmoid(z):\n",
    "    # TODO: implement stable sigmoid\n",
    "    raise NotImplementedError\n",
    "\n",
    "def logreg_loss_and_grad(X, y, w, l2=0.0):\n",
    "    # TODO: return (loss, grad)\n",
    "    raise NotImplementedError\n",
    "\n",
    "def fit_logreg_gd(X, y, lr=0.1, n_steps=2000, l2=0.0):\n",
    "    # TODO: return (w, loss_history)\n",
    "    raise NotImplementedError\n",
    "\n",
    "def predict_proba_logreg(X, w):\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "def predict_logreg(X, w, threshold=0.5):\n",
    "    p = predict_proba_logreg(X, w)\n",
    "    return (p >= threshold).astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# kNN (scratch)\n",
    "# -----------------------------\n",
    "def knn_predict(X_train, y_train, X_query, k=5, weighted=False):\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "# -----------------------------\n",
    "# PCA (scratch)\n",
    "# -----------------------------\n",
    "def pca_fit(X, n_components):\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "def pca_transform(X, pca_model):\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "# -----------------------------\n",
    "# k-means (scratch)\n",
    "# -----------------------------\n",
    "def kmeans_fit(X, k, n_init=5, max_iter=200, tol=1e-4, random_state=42):\n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "def kmeans_predict(X, centroids):\n",
    "    # TODO\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba2c56",
   "metadata": {},
   "source": [
    "## 7) Unsupervised learning: PCA + k-means \n",
    "\n",
    "### Hard requirements (outputs)\n",
    "#### PCA\n",
    "1. Plot cumulative explained variance for at least 1–20 components (or up to d)\n",
    "2. Choose `n_components` and justify in **3–6 lines**\n",
    "3. Create a 2D PCA projection and plot a scatter colored by `Occupancy`\n",
    "\n",
    "#### k-means\n",
    "4. Run k-means for at least **4** values of k (e.g., 2–6)\n",
    "5. Plot silhouette score vs k\n",
    "6. Plot 2D PCA scatter colored by cluster labels for your chosen k\n",
    "\n",
    "#### Interpretation (required text)\n",
    "Write **5–10 lines** explaining:\n",
    "- whether clusters seem to match labels,\n",
    "- what this implies for supervised learning difficulty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9192df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# TODO: PCA explained variance + PCA2 scatter\n",
    "# TODO: k-means sweep + silhouette plot + cluster scatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12004920",
   "metadata": {},
   "source": [
    "### Unsupervised interpretation (required: 5–10 lines)\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca36ed9",
   "metadata": {},
   "source": [
    "## 8) Supervised learning: model selection + overfitting evidence \n",
    "\n",
    "### Required model families\n",
    "**From scratch**\n",
    "- Logistic Regression\n",
    "- kNN\n",
    "\n",
    "**sklearn allowed**\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- MLPClassifier\n",
    "\n",
    "### Hard requirements (outputs)\n",
    "#### A) Comparison table (single table)\n",
    "Create one table showing the **best validation** performance for each model family.\n",
    "Table must include:\n",
    "- Train accuracy, Train F1\n",
    "- Validation accuracy, Validation F1\n",
    "- Chosen hyperparameters (as a short text field)\n",
    "\n",
    "#### B) Overfitting/complexity curves (minimum 4 figures)\n",
    "You must show train vs validation F1 curves for:\n",
    "1. Logistic regression vs `l2` (≥ 6 values)\n",
    "2. kNN vs `k` (≥ 8 values)\n",
    "3. Decision Tree vs `max_depth` (≥ 8 values)\n",
    "4. Random Forest vs `n_estimators` (≥ 6 values)\n",
    "\n",
    "MLP:\n",
    "- `TRACK=\"3hp\"`: recommended (at least 3 architectures)\n",
    "- `TRACK=\"4.5hp\"`: required (see Step 10)\n",
    "\n",
    "#### C) Required text (8–15 lines)\n",
    "Explain:\n",
    "- which models overfit and where,\n",
    "- why you selected your final model family,\n",
    "- which hyperparameters mattered most.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d73725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def metrics_row(name, config, ytr, yhat_tr, yva, yhat_va):\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"config\": config,\n",
    "        \"acc_train\": accuracy_score(ytr, yhat_tr),\n",
    "        \"f1_train\": f1_score(ytr, yhat_tr),\n",
    "        \"acc_val\": accuracy_score(yva, yhat_va),\n",
    "        \"f1_val\": f1_score(yva, yhat_va),\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "\n",
    "# TODO: implement sweeps and plots for:\n",
    "# - LogReg: l2 grid (>= 6) -> train/val F1 curve + loss plot at least once\n",
    "# - kNN: k grid (>= 8) -> train/val F1 curve\n",
    "# - DT: depth grid (>= 8) -> train/val F1 curve\n",
    "# - RF: n_estimators grid (>= 6) -> train/val F1 curve\n",
    "# - MLP: try >= 3 architectures (recommended 3hp, required 4.5hp)\n",
    "\n",
    "comparison = None  # pd.DataFrame(rows)\n",
    "# display(comparison.sort_values(\"f1_val\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741a3e6",
   "metadata": {},
   "source": [
    "### Supervised interpretation (required: 8–15 lines)\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3ba16",
   "metadata": {},
   "source": [
    "## 9) Final model evaluation on the test set\n",
    "\n",
    "### Hard requirements (outputs)\n",
    "1. Select one final model family and hyperparameters (based on validation)\n",
    "2. Retrain on **train + validation**\n",
    "3. Evaluate on **test**:\n",
    "   - Print Accuracy and F1\n",
    "   - Confusion matrix plot\n",
    "\n",
    "4. If probability scores are available:\n",
    "   - ROC curve plot\n",
    "   - Precision-Recall curve plot\n",
    "\n",
    "### Required text (5–10 lines)\n",
    "Explain why you think this test performance is a fair estimate (or not).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "# Combine train + val\n",
    "Xtv_np = np.vstack([Xtr_np, Xva_np])\n",
    "ytv = np.concatenate([y_train.values, y_val.values])\n",
    "\n",
    "FINAL_MODEL_NAME = \"RF\"  # choose: \"LogReg\", \"kNN\", \"DT\", \"RF\", \"MLP\"\n",
    "\n",
    "# TODO: train final model on Xtv_np, ytv\n",
    "# TODO: evaluate on Xte_np, y_test\n",
    "# TODO: confusion matrix plot\n",
    "# TODO: ROC/PR if you have scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f6c5b3",
   "metadata": {},
   "source": [
    "### Test-set discussion (required: 5–10 lines)\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e24d9",
   "metadata": {},
   "source": [
    "## 10) 4.5 hp Extension — Advanced Model Analysis (required if `TRACK=\"4.5hp\"`)\n",
    "\n",
    "This step is what justifies the extra 1.5 hp. It focuses on:\n",
    "- stability of results,\n",
    "- systematic model selection,\n",
    "- bias–variance reasoning,\n",
    "- robustness,\n",
    "- interpretation.\n",
    "\n",
    "If `TRACK=\"3hp\"` you should skip this entire step.\n",
    "\n",
    "---\n",
    "\n",
    "### 10A) Stratified Cross-Validation (required)\n",
    "\n",
    "You must perform **Stratified K-Fold Cross-Validation** (k ≥ 5).\n",
    "\n",
    "**Minimum requirements**\n",
    "- Evaluate at least **3** model families using cross-validation:\n",
    "  - one from-scratch model (LogReg or kNN)\n",
    "  - one tree-based model (DT or RF)\n",
    "  - MLPClassifier\n",
    "- For each model family, report:\n",
    "  - mean F1 across folds\n",
    "  - std(F1) across folds\n",
    "\n",
    "**Outputs we expect**\n",
    "1. A table: `model`, `config`, `mean_f1`, `std_f1`\n",
    "2. A short text (5–10 lines): which model seems most stable and why?\n",
    "\n",
    "---\n",
    "\n",
    "### 10B) Bias–Variance behaviour (required)\n",
    "\n",
    "For at least **2 model families**, produce a complexity curve and interpret it.\n",
    "\n",
    "**Minimum requirements**\n",
    "- Choose 2 of: LogReg, kNN, DT, RF, MLP\n",
    "- For each chosen family:\n",
    "  - plot train vs validation F1 vs a complexity hyperparameter (e.g., l2, k, depth, trees, alpha, architecture)\n",
    "  - in 8–12 lines, explicitly identify:\n",
    "    - underfitting (high bias) region\n",
    "    - overfitting (high variance) region\n",
    "    - your selected trade-off point\n",
    "\n",
    "**Outputs we expect**\n",
    "- 2 figures (or more) with train+val curves\n",
    "- 2 short texts (8–12 lines each)\n",
    "\n",
    "---\n",
    "\n",
    "### 10C) Structured hyperparameter search (required)\n",
    "\n",
    "You must run a structured hyperparameter search for at least **2** model families.\n",
    "This should be larger than the sweeps in Step 8.\n",
    "\n",
    "**Minimum requirements**\n",
    "- ≥ 20 configurations per model family\n",
    "- store all results in a DataFrame\n",
    "- show top 10 configurations sorted by validation F1\n",
    "- include at least one visualization of the search results:\n",
    "  - heatmap (recommended for 2D grids) OR\n",
    "  - line plot OR\n",
    "  - scatter plot\n",
    "\n",
    "**Outputs we expect**\n",
    "- result DataFrames (full + top10)\n",
    "- at least 1 search visualization per tuned family\n",
    "\n",
    "---\n",
    "\n",
    "### 10D) Threshold optimization (required if final model outputs probabilities)\n",
    "\n",
    "If your final model can output probabilities (LogReg/MLP/RF usually can), do threshold tuning.\n",
    "\n",
    "**Minimum requirements**\n",
    "- sweep thresholds from 0.05 to 0.95\n",
    "- plot F1 vs threshold\n",
    "- show confusion matrix at threshold 0.5 and at your chosen threshold\n",
    "- write 5–10 lines explaining precision/recall trade-off\n",
    "\n",
    "---\n",
    "\n",
    "### 10E) Robustness analysis (choose ONE, required)\n",
    "\n",
    "Choose one of the options:\n",
    "\n",
    "**Option 1: repeated splits**\n",
    "- repeat the full training procedure for 5 random seeds\n",
    "- report mean ± std of test F1\n",
    "\n",
    "**Option 2: noise sensitivity**\n",
    "- add Gaussian noise to numeric features (several noise levels)\n",
    "- plot F1 vs noise level\n",
    "- interpret sensitivity in 5–10 lines\n",
    "\n",
    "---\n",
    "\n",
    "### 10F) Feature importance / interpretation (required)\n",
    "\n",
    "For your final selected model:\n",
    "- If RF: use `feature_importances_`\n",
    "- Otherwise: use permutation importance\n",
    "\n",
    "**Minimum requirements**\n",
    "- plot top 10 most important features\n",
    "- write 8–12 lines:\n",
    "  - do these features align with your EDA?\n",
    "  - any suspicious/leakage features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRACK != \"4.5hp\":\n",
    "    print(\"TRACK is 3hp — skipping Step 10.\")\n",
    "else:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.inspection import permutation_importance\n",
    "\n",
    "    # TODO 10A: cross-validation table (mean±std F1)\n",
    "    # TODO 10B: bias-variance interpretation (2 families)\n",
    "    # TODO 10C: structured hyperparameter search (>=20 configs per family, >=2 families)\n",
    "    # TODO 10D: threshold sweep if probabilistic final model\n",
    "    # TODO 10E: robustness (choose 1)\n",
    "    # TODO 10F: feature importance plot + interpretation\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a77684f",
   "metadata": {},
   "source": [
    "## 11) Final discussion (required)\n",
    "\n",
    "Write concise answers (bullet points ok). Minimum **10–20 lines** total.\n",
    "\n",
    "1. Which model did you select and why? (validation performance + overfitting evidence + complexity)\n",
    "2. What was the biggest source of overfitting, and how did you address it?\n",
    "3. What did unsupervised analysis (PCA + k-means) tell you about the data?\n",
    "4. What would you do next with more time or more data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c3443",
   "metadata": {},
   "source": [
    "### Final discussion\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56147b2f",
   "metadata": {},
   "source": [
    "## 12) Submission checklist (what we will verify)\n",
    "\n",
    "### Cleaning (Step 3)\n",
    "- [ ] Missing-values table before/after\n",
    "- [ ] Dtype table before/after\n",
    "- [ ] ≥ 4 plots showing issues and improvements\n",
    "- [ ] Target is binary {0,1} and no NaNs in modelling features\n",
    "- [ ] Cleaning summary (5–10 lines)\n",
    "\n",
    "### EDA (Step 4)\n",
    "- [ ] Class balance table\n",
    "- [ ] ≥ 6 plots + correlation visualization\n",
    "- [ ] EDA summary (5–10 lines)\n",
    "\n",
    "### Unsupervised (Step 7)\n",
    "- [ ] PCA explained variance plot\n",
    "- [ ] PCA-2D scatter colored by Occupancy\n",
    "- [ ] Silhouette vs k (≥ 4 values)\n",
    "- [ ] PCA-2D scatter colored by k-means clusters\n",
    "- [ ] Unsupervised interpretation (5–10 lines)\n",
    "\n",
    "### Supervised (Step 8)\n",
    "- [ ] Complexity curves: LogReg, kNN, DT, RF (train vs val shown)\n",
    "- [ ] Comparison table of best models (train + val metrics)\n",
    "- [ ] Supervised interpretation (8–15 lines)\n",
    "\n",
    "### Final evaluation (Step 9)\n",
    "- [ ] Test metrics printed (Accuracy, F1)\n",
    "- [ ] Confusion matrix plot\n",
    "- [ ] ROC + PR curves if probabilities available\n",
    "- [ ] Test discussion (5–10 lines)\n",
    "\n",
    "### 4.5hp only (Step 10)\n",
    "- [ ] 10A: CV table mean±std F1 + short text\n",
    "- [ ] 10B: bias–variance analysis for 2 families\n",
    "- [ ] 10C: structured search ≥20 configs for ≥2 families + visualizations\n",
    "- [ ] 10D: threshold tuning if probabilistic final model\n",
    "- [ ] 10E: robustness analysis\n",
    "- [ ] 10F: feature importance plot + interpretation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
