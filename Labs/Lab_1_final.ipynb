{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8a6580",
   "metadata": {},
   "source": [
    "# Lab 1 — Data Cleaning + Linear Regression from Scratch (Normal Equation & Gradient Descent)\n",
    "\n",
    "**Course:** Applied Machine Learning  \n",
    "**Duration:** ~2 hours (in-lab)\n",
    "\n",
    "## Learning goals\n",
    "By the end of this lab, you should be able to:\n",
    "- Diagnose realistic data issues using summaries and plots.\n",
    "- Fix issues column-by-column and verify that your fixes work.\n",
    "- Build **univariate** and **multivariate** linear regression models.\n",
    "- Solve univariate linear regression using:\n",
    "  - the **Normal Equation** (closed form)\n",
    "  - **Gradient Descent** (iterative optimization)\n",
    "- Compare model performance on:\n",
    "  - corrupted raw data\n",
    "  - corrupted **cleaned** data\n",
    "  - clean “oracle” data\n",
    "\n",
    "> **Important:** In the first half of the lab, you must work **only** with the corrupted dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0f945",
   "metadata": {},
   "source": [
    "## 0) Setup (runs in Colab / local)\n",
    "\n",
    "This notebook is **self-contained**:\n",
    "- If `Power_plant.csv` is available locally, we use it.\n",
    "- Otherwise, we download the Combined Cycle Power Plant dataset from UCI (zip) and load it.\n",
    "\n",
    "We then create two CSV files:\n",
    "- `power_plant_clean.csv` (oracle)\n",
    "- `power_plant_corrupt.csv` (contains realistic corruption you must fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25f5236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local file: Power_plant.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>V</th>\n",
       "      <th>AP</th>\n",
       "      <th>RH</th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.34</td>\n",
       "      <td>40.77</td>\n",
       "      <td>1010.84</td>\n",
       "      <td>90.01</td>\n",
       "      <td>480.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.64</td>\n",
       "      <td>58.49</td>\n",
       "      <td>1011.40</td>\n",
       "      <td>74.20</td>\n",
       "      <td>445.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.74</td>\n",
       "      <td>56.90</td>\n",
       "      <td>1007.15</td>\n",
       "      <td>41.91</td>\n",
       "      <td>438.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.07</td>\n",
       "      <td>49.69</td>\n",
       "      <td>1007.22</td>\n",
       "      <td>76.79</td>\n",
       "      <td>453.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.80</td>\n",
       "      <td>40.66</td>\n",
       "      <td>1017.13</td>\n",
       "      <td>97.20</td>\n",
       "      <td>464.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AT      V       AP     RH      PE\n",
       "0   8.34  40.77  1010.84  90.01  480.48\n",
       "1  23.64  58.49  1011.40  74.20  445.75\n",
       "2  29.74  56.90  1007.15  41.91  438.76\n",
       "3  19.07  49.69  1007.22  76.79  453.09\n",
       "4  11.80  40.66  1017.13  97.20  464.43"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you run this in Google Colab, you may want to install dependencies (usually already installed):\n",
    "# !pip -q install pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "import os, zipfile, urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If you run this in Google Colab, you may want to install dependencies (usually already installed):\n",
    "# !pip -q install pandas numpy matplotlib scikit-learn openpyxl\n",
    "\n",
    "import os, zipfile, urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "# Where the rest of the lab expects the \"clean/original\" dataset to be:\n",
    "DATA_LOCAL = \"Power_plant.csv\"          # stable local name used throughout the lab\n",
    "CLEAN_OUT  = \"power_plant_clean.csv\"    # optional alias if you also want this file\n",
    "\n",
    "UCI_ZIP_URL  = \"https://archive.ics.uci.edu/static/public/294/combined+cycle+power+plant.zip\"\n",
    "UCI_ZIP_PATH = \"ccpp.zip\"\n",
    "\n",
    "EXPECTED = [\"AT\", \"V\", \"AP\", \"RH\", \"PE\"]\n",
    "\n",
    "def ensure_openpyxl():\n",
    "    # Defensive: if openpyxl is missing, install it (Colab usually has it)\n",
    "    try:\n",
    "        import openpyxl  # noqa: F401\n",
    "    except Exception:\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"openpyxl\"])\n",
    "\n",
    "def extract_best_table_file_from_zip(zip_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the best candidate dataset file from the zip and return the extracted path.\n",
    "    Prefers:\n",
    "      1) Folds5x2_pp.xlsx (or similarly named)\n",
    "      2) any .xlsx\n",
    "      3) Folds5x2_pp.csv\n",
    "      4) any .csv\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        names = z.namelist()\n",
    "\n",
    "        def pick(preferred_substr: str, exts: tuple[str, ...]):\n",
    "            cand = [n for n in names if n.lower().endswith(exts)]\n",
    "            pref = [n for n in cand if preferred_substr in n.lower()]\n",
    "            return (pref[0] if pref else (cand[0] if cand else None))\n",
    "\n",
    "        chosen = (\n",
    "            pick(\"folds5x2_pp\", (\".xlsx\",))\n",
    "            or pick(\"\", (\".xlsx\",))\n",
    "            or pick(\"folds5x2_pp\", (\".csv\",))\n",
    "            or pick(\"\", (\".csv\",))\n",
    "        )\n",
    "\n",
    "        if chosen is None:\n",
    "            raise RuntimeError(f\"No .xlsx or .csv found in zip. Contents:\\n{names}\")\n",
    "\n",
    "        print(\"Extracting:\", chosen)\n",
    "        z.extract(chosen, \".\")\n",
    "        return chosen  # note: may include subfolder, e.g. \"CCPP/Folds5x2_pp.xlsx\"\n",
    "\n",
    "def load_table(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a table from .xlsx or .csv, normalize columns to EXPECTED when possible.\n",
    "    \"\"\"\n",
    "    if path.lower().endswith(\".xlsx\"):\n",
    "        ensure_openpyxl()\n",
    "        df = pd.read_excel(path)\n",
    "    else:\n",
    "        # robust CSV read\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Some versions have whitespace in headers\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    # If expected columns exist, keep them in correct order\n",
    "    if all(c in df.columns for c in EXPECTED):\n",
    "        return df[EXPECTED].copy()\n",
    "\n",
    "    # Otherwise: sometimes Excel has unnamed columns or different header row.\n",
    "    # Try a simple recovery: if there are 5 numeric columns, assume order is AT,V,AP,RH,PE.\n",
    "    if df.shape[1] >= 5:\n",
    "        df5 = df.iloc[:, :5].copy()\n",
    "        df5.columns = EXPECTED\n",
    "        return df5\n",
    "\n",
    "    raise RuntimeError(f\"Could not match expected columns {EXPECTED}. Got columns: {df.columns.tolist()}\")\n",
    "\n",
    "def ensure_clean_dataset() -> pd.DataFrame:\n",
    "    # 1) Use local file if present\n",
    "    if os.path.exists(DATA_LOCAL):\n",
    "        print(\"Using local file:\", DATA_LOCAL)\n",
    "        df = pd.read_csv(DATA_LOCAL, encoding=\"utf-8-sig\")\n",
    "        return df\n",
    "\n",
    "    # 2) Otherwise download zip from UCI (if needed)\n",
    "    if not os.path.exists(UCI_ZIP_PATH):\n",
    "        print(\"Downloading dataset from UCI...\")\n",
    "        urllib.request.urlretrieve(UCI_ZIP_URL, UCI_ZIP_PATH)\n",
    "\n",
    "    # 3) Extract best file (xlsx/csv)\n",
    "    extracted_rel = extract_best_table_file_from_zip(UCI_ZIP_PATH)\n",
    "\n",
    "    # 4) Load it\n",
    "    if not os.path.exists(extracted_rel):\n",
    "        # extremely defensive; extract should have created it\n",
    "        raise RuntimeError(f\"Extracted file not found on disk: {extracted_rel}\")\n",
    "\n",
    "    df = load_table(extracted_rel)\n",
    "\n",
    "    # 5) Save to stable filenames that the rest of the lab expects\n",
    "    df.to_csv(DATA_LOCAL, index=False)\n",
    "    df.to_csv(CLEAN_OUT, index=False)\n",
    "\n",
    "    print(\"Saved:\", DATA_LOCAL, \"and\", CLEAN_OUT)\n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "    return df\n",
    "\n",
    "df_clean = ensure_clean_dataset()\n",
    "\n",
    "# Final sanity check\n",
    "assert all(c in df_clean.columns for c in EXPECTED), f\"Expected columns {EXPECTED}, got {df_clean.columns.tolist()}\"\n",
    "\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4857c",
   "metadata": {},
   "source": [
    "### Cleaning Tool B — Fix obvious physical constraints\n",
    "\n",
    "Engineering priors:\n",
    "- Relative humidity `RH` should be in **[0, 100]**\n",
    "- Exhaust vacuum `V` should be **non-negative**\n",
    "\n",
    "**TODO:** implement one or more fixes:\n",
    "- Fix unit mistakes (if RH appears in 0..1)\n",
    "- Replace impossible values with NaN (so they can be imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1222d",
   "metadata": {},
   "source": [
    "### Cleaning Tool B — Fix obvious physical constraints\n",
    "\n",
    "Engineering priors:\n",
    "- Relative humidity `RH` should be in **[0, 100]**\n",
    "- Exhaust vacuum `V` should be **non-negative**\n",
    "\n",
    "**TODO:** implement one or more fixes:\n",
    "- Fix unit mistakes (if RH appears in 0..1)\n",
    "- Replace impossible values with NaN (so they can be imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7f186",
   "metadata": {},
   "source": [
    "## 1) Create the *corrupted* dataset \n",
    "\n",
    "We create a corrupted copy to simulate real-world issues:\n",
    "- **Non-numeric values** in numeric columns (`\"1010,84\"`, `\"N/A\"`, whitespace)\n",
    "- **Missing values**\n",
    "- **Impossible ranges** (e.g., RH > 100, negative vacuum)\n",
    "- **Outliers** (rare spikes)\n",
    "\n",
    "The goal is to detect and fix these issues **without being told which columns are affected**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corrupt_copy(df, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    d = df.copy(deep=True)\n",
    "    n = len(d)\n",
    "\n",
    "    # --- 1) AT: missing + outliers + \"N/A\" strings (AT is informative for PE)\n",
    "    m_nan = rng.random(n) < 0.05\n",
    "    d.loc[m_nan, \"AT\"] = np.nan\n",
    "\n",
    "    m_na = rng.random(n) < 0.02\n",
    "    d.loc[m_na, \"AT\"] = np.nan  # we'll later also inject a string in a separate step\n",
    "\n",
    "    m_out = rng.random(n) < 0.02\n",
    "    d.loc[m_out, \"AT\"] = d.loc[m_out, \"AT\"] + rng.choice([25.0, -20.0, 40.0], size=m_out.sum())\n",
    "\n",
    "    # Inject some \"N/A\" and whitespace strings in AT (forces object dtype later)\n",
    "    m_str = rng.random(n) < 0.01\n",
    "    d.loc[m_str, \"AT\"] = d.loc[m_str, \"AT\"].map(lambda x: \" N/A \" if rng.random() < 0.5 else f\" {x:.2f} \")\n",
    "\n",
    "    # --- 2) AP: decimal comma strings (e.g., \"1010,84\")\n",
    "    m_comma = rng.random(n) < 0.03\n",
    "    d.loc[m_comma, \"AP\"] = d.loc[m_comma, \"AP\"].map(lambda x: f\"{x:.2f}\".replace(\".\", \",\"))\n",
    "\n",
    "    # --- 3) RH: unit mistake and impossible values\n",
    "    m_unit = rng.random(n) < 0.03\n",
    "    d.loc[m_unit, \"RH\"] = d.loc[m_unit, \"RH\"] / 100.0  # 0..1 instead of 0..100\n",
    "\n",
    "    m_hi = rng.random(n) < 0.01\n",
    "    d.loc[m_hi, \"RH\"] = d.loc[m_hi, \"RH\"] + 60.0       # >100\n",
    "\n",
    "    # --- 4) V: negative sign mistakes + mild noise\n",
    "    m_neg = rng.random(n) < 0.02\n",
    "    d.loc[m_neg, \"V\"] = -d.loc[m_neg, \"V\"]\n",
    "\n",
    "    d[\"V\"] = d[\"V\"] + rng.normal(0, 0.2, size=n)\n",
    "\n",
    "    return d\n",
    "\n",
    "df_corrupt = make_corrupt_copy(df_clean, seed=RANDOM_STATE)\n",
    "df_corrupt.to_csv(\"power_plant_corrupt.csv\", index=False)\n",
    "\n",
    "print(\"Saved two files:\")\n",
    "print(\" - power_plant_clean.csv\")\n",
    "print(\" - power_plant_corrupt.csv\")\n",
    "df_corrupt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399380a6",
   "metadata": {},
   "source": [
    "## 2) Work ONLY with the corrupted dataset (first half)\n",
    "\n",
    "Load the corrupted dataset and do quick diagnostics.\n",
    "\n",
    "### Task 2.1 — What looks wrong?\n",
    "Use:\n",
    "- `df.info()` and `df.describe(include=\"all\")`\n",
    "- Missingness table\n",
    "- Range checks (e.g., humidity should be between 0 and 100)\n",
    "- Plots (pairplot, histograms, boxplots)\n",
    "\n",
    "Write down at least **3 concrete issues** you suspect exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"power_plant_corrupt.csv\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n--- info() ---\")\n",
    "display(df.info())\n",
    "\n",
    "print(\"\\n--- describe(include='all') ---\")\n",
    "display(df.describe(include=\"all\"))\n",
    "\n",
    "# Missingness\n",
    "missing = df.isna().mean().sort_values(ascending=False)\n",
    "display(pd.DataFrame({\"missing_rate\": missing}))\n",
    "\n",
    "# Quick range checks (some will fail if columns are non-numeric)\n",
    "def safe_to_numeric(s):\n",
    "    return pd.to_numeric(s.astype(str).str.strip().str.replace(\",\", \".\", regex=False), errors=\"coerce\")\n",
    "\n",
    "def coerce_numeric_series(s):\n",
    "    # Make strings, strip, replace decimal comma, set known NA tokens to NaN\n",
    "    s2 = s.astype(str).str.strip()\n",
    "    s2 = s2.replace({\"N/A\": np.nan, \"na\": np.nan, \"NA\": np.nan, \"\": np.nan, \"nan\": np.nan})\n",
    "    s2 = s2.str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(s2, errors=\"coerce\")\n",
    "\n",
    "\n",
    "tmp = df.copy()\n",
    "for c in [\"AT\",\"V\",\"AP\",\"RH\",\"PE\"]:\n",
    "    tmp[c] = safe_to_numeric(tmp[c])\n",
    "\n",
    "checks = {\n",
    "    \"V_negative_count\": int((tmp[\"V\"] < 0).sum()),\n",
    "    \"RH_out_of_range_count\": int(((tmp[\"RH\"] < 0) | (tmp[\"RH\"] > 100)).sum()),\n",
    "    \"AT_extreme_count_(< -10 or > 60)\": int(((tmp[\"AT\"] < -10) | (tmp[\"AT\"] > 60)).sum()),\n",
    "    \"non_numeric_entries_any_feature\": int(tmp[[\"AT\",\"V\",\"AP\",\"RH\"]].isna().sum().sum() - df[[\"AT\",\"V\",\"AP\",\"RH\"]].isna().sum().sum()),\n",
    "}\n",
    "display(pd.DataFrame([checks]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d71cd",
   "metadata": {},
   "source": [
    "### Task 2.2 — Visual diagnostics\n",
    "\n",
    "The plots below should help you spot:\n",
    "- strange distributions (e.g., impossible values)\n",
    "- outliers\n",
    "- broken linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6763bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot (sample to keep it fast)\n",
    "# Scatter-matrix (pairplot alternative without seaborn)\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "sample = df.sample(800, random_state=RANDOM_STATE)\n",
    "for c in [\"AT\",\"V\",\"AP\",\"RH\",\"PE\"]:\n",
    "    sample[c] = safe_to_numeric(sample[c])\n",
    "\n",
    "axes = scatter_matrix(sample[[\"AT\",\"V\",\"AP\",\"RH\",\"PE\"]], figsize=(10,10), diagonal=\"hist\", alpha=0.6)\n",
    "plt.suptitle(\"Scatter matrix (sampled)\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Histograms + boxplots (numeric coercion)\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "cols = [\"AT\",\"V\",\"AP\",\"RH\"]\n",
    "for i, c in enumerate(cols):\n",
    "    ax = axes[0, i]\n",
    "    ax.hist(tmp[c].dropna().values, bins=40)\n",
    "    ax.set_title(f\"{c} histogram\")\n",
    "for i, c in enumerate(cols):\n",
    "    ax = axes[1, i]\n",
    "    ax.boxplot(tmp[c].dropna().values, vert=True)\n",
    "    ax.set_title(f\"{c} boxplot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter AT vs PE (often strongest single feature)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(tmp[\"AT\"], tmp[\"PE\"], s=8)\n",
    "plt.xlabel(\"AT\")\n",
    "plt.ylabel(\"PE\")\n",
    "plt.title(\"AT vs PE (corrupted, coerced to numeric)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946838e",
   "metadata": {},
   "source": [
    "## 3) Cleaning, step-by-step \n",
    "\n",
    "You will apply **three generic cleaning tools**.  \n",
    "Your job is to decide **which column(s)** they should apply to, and **verify** using prints/plots that your fix worked.\n",
    "\n",
    "You should see:\n",
    "- fewer NaNs after coercion/imputation\n",
    "- fewer impossible values (e.g., RH outside 0–100, negative V)\n",
    "- improved regression performance later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ec5bd",
   "metadata": {},
   "source": [
    "### Cleaning Tool A — Convert to numeric robustly\n",
    "\n",
    "Problem pattern:\n",
    "- numeric values stored as strings\n",
    "- decimal comma `\"1010,84\"`\n",
    "- whitespace `\" 1010.84 \"`\n",
    "- `\"N/A\"` / `\"na\"` entries\n",
    "\n",
    "**TODO:** choose which columns to coerce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = pd.read_csv(\"power_plant_corrupt.csv\")\n",
    "\n",
    "def coerce_numeric_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert a column to numeric in a robust way:\n",
    "    - strip whitespace\n",
    "    - replace decimal comma with dot\n",
    "    - map common NA tokens to NaN\n",
    "    - coerce invalid values to NaN\n",
    "    \"\"\"\n",
    "    s2 = s.astype(str).str.strip()\n",
    "    s2 = s2.replace({\"N/A\": np.nan, \"na\": np.nan, \"NA\": np.nan, \"\": np.nan, \"nan\": np.nan})\n",
    "    s2 = s2.str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(s2, errors=\"coerce\")\n",
    "\n",
    "# TODO:\n",
    "# Based on the inspection, select columns that should be numeric\n",
    "# but currently contain non-numeric entries.\n",
    "cols_to_coerce = [\"AT\", \"V\", \"AP\", \"RH\", \"PE\"]  # edit if needed\n",
    "\n",
    "for c in cols_to_coerce:\n",
    "    df_work[c] = coerce_numeric_series(df_work[c])\n",
    "\n",
    "print(\"Dtypes after coercion:\")\n",
    "display(df_work.dtypes)\n",
    "\n",
    "print(\"NaN counts after coercion (features):\")\n",
    "display(df_work[[\"AT\",\"V\",\"AP\",\"RH\"]].isna().sum().to_frame(\"nan_count\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d170c32",
   "metadata": {},
   "source": [
    "### Cleaning Tool B — Fix obvious physical constraints\n",
    "\n",
    "Engineering priors:\n",
    "- Relative humidity `RH` should be in **[0, 100]**\n",
    "- Exhaust vacuum `V` should be **non-negative**\n",
    "\n",
    "**TODO:** implement one or more fixes:\n",
    "- Fix unit mistakes (if RH appears in 0..1)\n",
    "- Replace impossible values with NaN (so they can be imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric copy for checks (do NOT modify num directly)\n",
    "num = df_work.copy()\n",
    "for c in [\"AT\",\"V\",\"AP\",\"RH\",\"PE\"]:\n",
    "    num[c] = coerce_numeric_series(num[c])\n",
    "\n",
    "print(\"Before constraints:\")\n",
    "display(pd.DataFrame([{\n",
    "    \"V_negative\": int((num[\"V\"] < 0).sum()),\n",
    "    \"RH_out_of_range\": int(((num[\"RH\"] < 0) | (num[\"RH\"] > 100)).sum()),\n",
    "    \"RH_leq_1_fraction\": float((num[\"RH\"] <= 1.0).mean())\n",
    "}]))\n",
    "\n",
    "# --- Apply constraint fixes to df_work ---\n",
    "\n",
    "# TODO 1:\n",
    "# If RH seems to be in [0, 1], convert it to percentage [0, 100].\n",
    "# (We use a simple heuristic threshold.)\n",
    "if float((num[\"RH\"] <= 1.0).mean()) > 0.5:\n",
    "    df_work[\"RH\"] = df_work[\"RH\"] * 100\n",
    "\n",
    "# TODO 2:\n",
    "# Replace physically invalid values with NaN:\n",
    "# - negative V\n",
    "# - RH outside [0, 100]\n",
    "df_work.loc[df_work[\"V\"] < 0, \"V\"] = np.nan\n",
    "df_work.loc[(df_work[\"RH\"] < 0) | (df_work[\"RH\"] > 100), \"RH\"] = np.nan\n",
    "\n",
    "# Recompute checks\n",
    "num2 = df_work.copy()\n",
    "for c in [\"AT\",\"V\",\"AP\",\"RH\",\"PE\"]:\n",
    "    num2[c] = coerce_numeric_series(num2[c])\n",
    "\n",
    "print(\"After constraints:\")\n",
    "display(pd.DataFrame([{\n",
    "    \"V_negative\": int((num2[\"V\"] < 0).sum()),\n",
    "    \"RH_out_of_range\": int(((num2[\"RH\"] < 0) | (num2[\"RH\"] > 100)).sum()),\n",
    "    \"RH_leq_1_fraction\": float((num2[\"RH\"] <= 1.0).mean())\n",
    "}]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf01d00",
   "metadata": {},
   "source": [
    "### Cleaning Tool C — Missing values + outliers\n",
    "\n",
    "Strategy:\n",
    "1) Convert all feature columns to numeric (coerce)\n",
    "2) Replace impossible values with NaN (from tool B)\n",
    "3) **Impute** missing values (median is a robust baseline)\n",
    "4) **Clip** extreme outliers using quantiles (winsorization)\n",
    "\n",
    "**TODO:** choose quantiles (e.g., 1%–99% or 0.5%–99.5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze a stable base for this step (after coercion + constraints)\n",
    "df_base_cleaning = df_work.copy()\n",
    "\n",
    "# Start fresh from the same base every time you run this cell\n",
    "df_work = df_base_cleaning.copy()\n",
    "\n",
    "print(\"NaNs before imputation:\")\n",
    "display(df_work[[\"AT\",\"V\",\"AP\",\"RH\"]].isna().sum().to_frame(\"nan_count\"))\n",
    "\n",
    "# Median imputation (features only)\n",
    "for c in [\"AT\",\"V\",\"AP\",\"RH\"]:\n",
    "    df_work[c] = df_work[c].fillna(df_work[c].median())\n",
    "\n",
    "# TODO:\n",
    "# Choose ONE clipping level below.\n",
    "# A) mild:    0.005, 0.995\n",
    "# B) default: 0.01,  0.99\n",
    "# C) strong:  0.05,  0.95\n",
    "q_low, q_high = 0.01, 0.99  # edit this line\n",
    "\n",
    "for c in [\"AT\",\"V\",\"AP\",\"RH\"]:\n",
    "    lo, hi = df_work[c].quantile([q_low, q_high])\n",
    "    df_work[c] = df_work[c].clip(lo, hi)\n",
    "\n",
    "print(\"NaNs after imputation:\")\n",
    "display(df_work[[\"AT\",\"V\",\"AP\",\"RH\"]].isna().sum().to_frame(\"nan_count\"))\n",
    "\n",
    "display(\n",
    "    df_work[[\"AT\",\"V\",\"AP\",\"RH\"]]\n",
    "    .describe()\n",
    "    .loc[[\"min\",\"max\",\"mean\",\"std\"]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa5d2d",
   "metadata": {},
   "source": [
    "## 4) Regression tasks (univariate + multivariate)\n",
    "\n",
    "You will now:\n",
    "1) Fit **univariate** linear regression using `AT → PE`  \n",
    "2) Fit **multivariate** linear regression using `[AT, V, AP, RH] → PE`\n",
    "\n",
    "\n",
    "To evaluate regression performance, we use the **Root Mean Squared Error (RMSE)**:\n",
    "\n",
    "$$\n",
    "\\mathrm{RMSE}\n",
    "=\n",
    "\\sqrt{\n",
    "\\frac{1}{m}\n",
    "\\sum_{i=1}^{m}\n",
    "\\left(\n",
    "\\hat{y}^{(i)} - y^{(i)}\n",
    "\\right)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $m$ is the number of samples,\n",
    "- $y^{(i)}$ is the true target value,\n",
    "- $\\hat{y}^{(i)}$ is the model prediction.\n",
    "\n",
    "RMSE measures the typical prediction error in the **same physical units as the target variable**.\n",
    "Lower RMSE indicates better predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc7cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Work dataset is your cleaned df_work (from above)\n",
    "df_model = df_work.copy()\n",
    "\n",
    "X_uni = df_model[[\"AT\"]].to_numpy()\n",
    "X_multi = df_model[[\"AT\",\"V\",\"AP\",\"RH\"]].to_numpy()\n",
    "y = df_model[[\"PE\"]].to_numpy()\n",
    "\n",
    "# Split ONCE (same rows for both models)\n",
    "idx = np.arange(len(df_model))\n",
    "idx_train, idx_test = train_test_split(idx, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "Xuni_train, Xuni_test = X_uni[idx_train], X_uni[idx_test]\n",
    "Xmul_train, Xmul_test = X_multi[idx_train], X_multi[idx_test]\n",
    "y_train, y_test       = y[idx_train], y[idx_test]\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).reshape(-1, 1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1, 1)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "print(\"Train size:\", len(y_train), \"Test size:\", len(y_test))\n",
    "\n",
    "# Standardize univariate AT using TRAIN statistics (GD stability)\n",
    "x_mu = float(Xuni_train.mean())\n",
    "x_sigma = float(Xuni_train.std()) if float(Xuni_train.std()) > 0 else 1.0\n",
    "\n",
    "Xuni_train_s = (Xuni_train - x_mu) / x_sigma\n",
    "Xuni_test_s  = (Xuni_test  - x_mu) / x_sigma\n",
    "\n",
    "# Design matrices for GD (bias + scaled x)\n",
    "m_train = Xuni_train_s.shape[0]\n",
    "m_test  = Xuni_test_s.shape[0]\n",
    "Xb_train_gd = np.hstack([np.ones((m_train,1)), Xuni_train_s])\n",
    "Xb_test_gd  = np.hstack([np.ones((m_test,1)),  Xuni_test_s])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b8a66",
   "metadata": {},
   "source": [
    "### 4.1 Univariate regression (Normal Equation) — **complete the missing steps**\n",
    "\n",
    "Model:\n",
    "$$\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "\n",
    "Design matrix with bias term:\n",
    "$$\n",
    "X_b =\n",
    "\\begin{bmatrix}\n",
    "1 & x^{(1)} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Normal equation:\n",
    "$$\n",
    "\\theta = (X_b^T X_b)^{-1} X_b^T y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Matrix operations in Python (NumPy)\n",
    "\n",
    "In NumPy, matrix operations closely follow the mathematical notation:\n",
    "\n",
    "- **Matrix multiplication** uses the `@` operator  \n",
    "  $$\n",
    "  AB \\;\\;\\leftrightarrow\\;\\; A \\ @ \\ B\n",
    "  $$\n",
    "\n",
    "- **Transpose** is accessed via `.T`  \n",
    "  $$\n",
    "  X_b^T \\;\\;\\leftrightarrow\\;\\; X_b.T\n",
    "  $$\n",
    "\n",
    "- **Matrix inverse** is computed using NumPy’s linear algebra module  \n",
    "  $$\n",
    "  (X_b^T X_b)^{-1} \\;\\;\\leftrightarrow\\;\\; \\texttt{np.linalg.inv(Xb.T @ Xb)}\n",
    "  $$\n",
    "\n",
    "Putting this together, the normal equation can be written in Python as:\n",
    "```python\n",
    "theta = np.linalg.inv(Xb.T @ Xb) @ (Xb.T @ y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "For problems with a large number of features, computing the normal equation is computationally expensive  \n",
    "(due to the matrix inverse) and can become numerically unstable.\n",
    "\n",
    "In practice, one instead uses **iterative optimization methods** (e.g. gradient descent) or  \n",
    "high-level implementations such as `LinearRegression` in `scikit-learn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e00595",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train, m_test = Xuni_train.shape[0], Xuni_test.shape[0]\n",
    "\n",
    "# TODO(1): Build the design matrices X_b\n",
    "# - Add a bias (intercept) column of ones\n",
    "# - The bias column should be the FIRST column\n",
    "#\n",
    "# Hint:\n",
    "#   Use np.ones(...) and np.hstack(...)\n",
    "Xb_train = None\n",
    "Xb_test  = None\n",
    "\n",
    "\n",
    "# Ensure column-vector shape for y\n",
    "y_train_col = y_train.reshape(-1, 1)\n",
    "y_test_col  = y_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# TODO(2): Compute the parameter vector theta using the normal equation\n",
    "#\n",
    "#   theta = (X_b^T X_b)^{-1} X_b^T y\n",
    "#\n",
    "# Hint:\n",
    "#   - Use @ for matrix multiplication\n",
    "#   - Use .T for transpose\n",
    "theta_ne = None\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "y_pred_uni = Xb_test @ theta_ne\n",
    "print(\"Normal Equation RMSE:\", rmse(y_test_col, y_pred_uni), \"\\nR^2:\", r2_score(y_test_col, y_pred_uni))\n",
    "\n",
    "\n",
    "# --- Visualization ---\n",
    "x_plot = Xb_train[:, 1]\n",
    "idx = np.argsort(x_plot)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(x_plot, y_train_col, label=\"Training data\")\n",
    "plt.plot(x_plot[idx], (Xb_train @ theta_ne)[idx], linewidth=2, label=\"Normal Eq fit\")\n",
    "plt.xlabel(\"AT\")\n",
    "plt.ylabel(\"PE\")\n",
    "plt.title(\"Univariate Linear Regression (Normal Equation)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9510fa0b",
   "metadata": {},
   "source": [
    "### 4.2 Multivariate linear regression (Normal Equation, no scikit-learn)\n",
    "\n",
    "We now predict electrical power output using four input features:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta_0 + \\theta_1 AT + \\theta_2 V + \\theta_3 AP + \\theta_4 RH\n",
    "$$\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{m \\times 4}$ contain the columns $[AT, V, AP, RH]$.  \n",
    "We add a bias (intercept) term to form the design matrix $X_b$:\n",
    "\n",
    "$$\n",
    "X_b =\n",
    "\\begin{bmatrix}\n",
    "1 & AT^{(1)} & V^{(1)} & AP^{(1)} & RH^{(1)}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "1 & AT^{(m)} & V^{(m)} & AP^{(m)} & RH^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Normal equation:\n",
    "$$\n",
    "\\theta = (X_b^T X_b)^{-1} X_b^T y\n",
    "$$\n",
    "\n",
    "**Important (no data leakage):**  \n",
    "If you standardize features, compute mean and std **on the training set only**, then apply the same transform to the test set.\n",
    "\n",
    "**TODO requirements**\n",
    "- Standardize the feature columns (train statistics only) *(recommended)*\n",
    "- Add the bias column to build $X_b$\n",
    "- Compute $\\theta$ using the normal equation (with matrix inverse)\n",
    "- Evaluate using RMSE and $R^2$\n",
    "- Plot **Predicted vs True** on the test set, and include the reference line $y=x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d98c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score  # metrics are fine; model must be from scratch\n",
    "\n",
    "# Shapes\n",
    "m_train, n_features = Xmul_train.shape\n",
    "m_test = Xmul_test.shape[0]\n",
    "\n",
    "# Ensure column vectors\n",
    "y_train_col = y_train.reshape(-1, 1)\n",
    "y_test_col  = y_test.reshape(-1, 1)\n",
    "\n",
    "# -----------------------------\n",
    "# TODO(1): Standardize features (recommended)\n",
    "# Compute mean/std from TRAIN only, then transform train and test.\n",
    "#\n",
    "# Hint:\n",
    "#   mu = ...\n",
    "#   sigma = ...\n",
    "#   Xs_train = ...\n",
    "#   Xs_test  = ...\n",
    "# -----------------------------\n",
    "Xs_train = None\n",
    "Xs_test  = None\n",
    "\n",
    "# -----------------------------\n",
    "# TODO(2): Add bias column to build design matrices\n",
    "# Xb = [1, standardized_features...]\n",
    "# -----------------------------\n",
    "Xb_train = None\n",
    "Xb_test  = None\n",
    "\n",
    "# -----------------------------\n",
    "# TODO(3): Normal equation using matrix inverse\n",
    "# theta = (Xb^T Xb)^{-1} Xb^T y\n",
    "#\n",
    "# Hint:\n",
    "#   A = Xb_train.T @ Xb_train\n",
    "#   b = Xb_train.T @ y_train_col\n",
    "#   theta = np.linalg.inv(A) @ b\n",
    "# -----------------------------\n",
    "theta = None\n",
    "\n",
    "# Predict\n",
    "y_pred_multi = Xb_test @ theta\n",
    "\n",
    "# Metrics\n",
    "print(\"Multivariate (from scratch) RMSE:\", rmse(y_test_col, y_pred_multi))\n",
    "print(\"Multivariate (from scratch) R^2:\", r2_score(y_test_col, y_pred_multi))\n",
    "# -----------------------------\n",
    "# Plot: Predicted vs True (test set)\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(y_test_col, y_pred_multi, alpha=0.7, label=\"Predictions\")\n",
    "\n",
    "# Reference line y = x\n",
    "minv = float(min(y_test_col.min(), y_pred_multi.min()))\n",
    "maxv = float(max(y_test_col.max(), y_pred_multi.max()))\n",
    "plt.plot([minv, maxv], [minv, maxv], linewidth=2, label=\"Perfect prediction (y = x)\", color=\"orange\")\n",
    "\n",
    "plt.xlabel(\"True PE\")\n",
    "plt.ylabel(\"Predicted PE\")\n",
    "plt.title(\"Multivariate Linear Regression: Predicted vs True (test set)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a6dbdc",
   "metadata": {},
   "source": [
    "### 4.3 Compare models visually (Predicted vs True)\n",
    "\n",
    "We compare two regression models trained on the **cleaned dataset** `df_work`:\n",
    "\n",
    "- **Univariate**: $AT \\rightarrow PE$\n",
    "- **Multivariate**: $[AT, V, AP, RH] \\rightarrow PE$\n",
    "\n",
    "For each model we report:\n",
    "- RMSE\n",
    "- $R^2$\n",
    "\n",
    "And we plot **Predicted vs True** on the test set, including the reference line $y=x$  \n",
    "(perfect predictions would lie exactly on this line).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebdbaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure consistent shapes ---\n",
    "y_true = np.asarray(y_test_col).reshape(-1, 1)\n",
    "y_uni  = np.asarray(y_pred_uni).reshape(-1, 1)\n",
    "y_mul  = np.asarray(y_pred_multi).reshape(-1, 1)\n",
    "\n",
    "# --- Metrics ---\n",
    "rmse_uni = rmse(y_true, y_uni)\n",
    "r2_uni   = r2_score(y_true, y_uni)\n",
    "\n",
    "rmse_mul = rmse(y_true, y_mul)\n",
    "r2_mul   = r2_score(y_true, y_mul)\n",
    "\n",
    "print(f\"Univariate (AT)            -> RMSE: {rmse_uni:.3f}, R^2: {r2_uni:.3f}\")\n",
    "print(f\"Multivariate (AT,V,AP,RH)  -> RMSE: {rmse_mul:.3f}, R^2: {r2_mul:.3f}\")\n",
    "\n",
    "# --- Plot: Predicted vs True (both models in one figure) ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(y_true, y_uni, alpha=0.6, color=\"red\",   label=\"Univariate (AT)\")\n",
    "plt.scatter(y_true, y_mul, alpha=0.6, color=\"green\", label=\"Multivariate (AT, V, AP, RH)\")\n",
    "\n",
    "minv = float(min(y_true.min(), y_uni.min(), y_mul.min()))\n",
    "maxv = float(max(y_true.max(), y_uni.max(), y_mul.max()))\n",
    "plt.plot([minv, maxv], [minv, maxv], linewidth=2, color=\"orange\", label=\"Perfect prediction (y = x)\")\n",
    "\n",
    "plt.xlabel(\"True PE\")\n",
    "plt.ylabel(\"Predicted PE\")\n",
    "plt.title(\"Predicted vs True — Univariate vs Multivariate (test set)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdadf0",
   "metadata": {},
   "source": [
    "## 5) Compare data quality: corrupted raw vs cleaned vs original\n",
    "\n",
    "This is the key scientific habit in applied ML:\n",
    "\n",
    "- define a **baseline**\n",
    "- change **one factor**\n",
    "- measure the effect on the **same test set**\n",
    "\n",
    "Here we keep the **model and evaluation protocol fixed** (same features, same preprocessing pipeline, same train/test split),\n",
    "and only change the **data quality**:\n",
    "\n",
    "1) **Corrupted raw**: minimal preparation (just make it runnable)\n",
    "2) **Corrupted cleaned**: your cleaned dataset (`df_work`)\n",
    "3) **Original**: the uncorrupted reference dataset\n",
    "\n",
    "We compare performance using **RMSE** and **$R^2$** on the same test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "FEATURES = [\"AT\", \"V\", \"AP\", \"RH\"]\n",
    "TARGET = \"PE\"\n",
    "\n",
    "# --- Helper: numeric coercion (reuse your existing function if already defined) ---\n",
    "def coerce_numeric_series(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(str).str.strip()\n",
    "    s2 = s2.replace({\"N/A\": np.nan, \"na\": np.nan, \"NA\": np.nan, \"\": np.nan, \"nan\": np.nan})\n",
    "    s2 = s2.str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(s2, errors=\"coerce\")\n",
    "\n",
    "def prepare_numeric(df_any: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df_any.copy()\n",
    "    for c in FEATURES + [TARGET]:\n",
    "        d[c] = coerce_numeric_series(d[c])\n",
    "    return d\n",
    "\n",
    "def rmse_np(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true).reshape(-1, 1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1, 1)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "# --- Load the three scenarios ---\n",
    "df_corrupt_raw = pd.read_csv(\"power_plant_corrupt.csv\")\n",
    "df_corrupt_raw = prepare_numeric(df_corrupt_raw)\n",
    "\n",
    "# \"Raw baseline\": only median-impute features so the pipeline can run\n",
    "df_corrupt_raw_base = df_corrupt_raw.copy()\n",
    "for c in FEATURES:\n",
    "    df_corrupt_raw_base[c] = df_corrupt_raw_base[c].fillna(df_corrupt_raw_base[c].median())\n",
    "\n",
    "df_cleaned = df_work.copy()  # your cleaned dataset\n",
    "df_original = pd.read_csv(\"power_plant_clean.csv\")  # uncorrupted reference\n",
    "df_original = prepare_numeric(df_original)\n",
    "\n",
    "# --- Ensure comparable indexing/length ---\n",
    "n = min(len(df_corrupt_raw_base), len(df_cleaned), len(df_original))\n",
    "df_corrupt_raw_base = df_corrupt_raw_base.iloc[:n].reset_index(drop=True)\n",
    "df_cleaned = df_cleaned.iloc[:n].reset_index(drop=True)\n",
    "df_original = df_original.iloc[:n].reset_index(drop=True)\n",
    "\n",
    "# --- Same split indices for fairness ---\n",
    "idx = np.arange(n)\n",
    "idx_train, idx_test = train_test_split(idx, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Fixed model+preprocessing for all scenarios\n",
    "pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "def fit_eval(df_any: pd.DataFrame, name: str):\n",
    "    X = df_any[FEATURES].to_numpy()\n",
    "    y = df_any[[TARGET]].to_numpy()\n",
    "\n",
    "    Xtr, Xte = X[idx_train], X[idx_test]\n",
    "    ytr, yte = y[idx_train], y[idx_test]\n",
    "\n",
    "    pipe.fit(Xtr, ytr.ravel())\n",
    "    pred = pipe.predict(Xte).reshape(-1, 1)\n",
    "\n",
    "    return {\n",
    "        \"Scenario\": name,\n",
    "        \"RMSE\": rmse_np(yte, pred),\n",
    "        \"R^2\": float(r2_score(yte, pred)),\n",
    "    }\n",
    "\n",
    "rows = [\n",
    "    fit_eval(df_corrupt_raw_base, \"Corrupted raw (minimal)\"),\n",
    "    fit_eval(df_cleaned, \"Corrupted cleaned (df_work)\"),\n",
    "    fit_eval(df_original, \"Original (uncorrupted)\"),\n",
    "]\n",
    "\n",
    "res = pd.DataFrame(rows)\n",
    "\n",
    "# Keep a pedagogical order (not sorted)\n",
    "order = [\"Corrupted raw (minimal)\", \"Corrupted cleaned (df_work)\", \"Original (uncorrupted)\"]\n",
    "res[\"Scenario\"] = pd.Categorical(res[\"Scenario\"], categories=order, ordered=True)\n",
    "res = res.sort_values(\"Scenario\").reset_index(drop=True)\n",
    "\n",
    "display(res)\n",
    "\n",
    "# --- Visualization: RMSE bars + annotate RMSE and R^2 ---\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "\n",
    "colors = [\"#d62728\", \"#2ca02c\", \"#1f77b4\"]  # red, green, blue\n",
    "bars = plt.bar(res[\"Scenario\"].astype(str), res[\"RMSE\"], color=colors)\n",
    "\n",
    "plt.ylabel(\"RMSE (lower is better)\")\n",
    "plt.title(\"Impact of data quality on model performance (same split, same model)\")\n",
    "plt.xticks(rotation=15, ha=\"right\")\n",
    "\n",
    "# --- FIX: add headroom on y-axis ---\n",
    "y_max = res[\"RMSE\"].max()\n",
    "plt.ylim(0, y_max * 1.20)   # 20% headroom for annotations\n",
    "\n",
    "# --- Annotate bars (slightly above each bar) ---\n",
    "for bar, rmse_val, r2_val in zip(bars, res[\"RMSE\"], res[\"R^2\"]):\n",
    "    h = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        h + 0.05 * y_max,   # offset ABOVE bar, relative to data scale\n",
    "        f\"RMSE={rmse_val:.2f}\\nR²={r2_val:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.grid(True, axis=\"y\", alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca842948",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "In this lab you built and evaluated a complete machine learning pipeline, from raw data inspection to model evaluation.  \n",
    "You saw that improving **data quality** can have a larger impact on performance than changing the model itself.  \n",
    "You also observed how adding relevant features improves predictions when models are evaluated fairly on the same test set.  \n",
    "The workflow you practiced here - understand the data, preprocess carefully, train, and evaluate - is central to all applied machine learning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matmod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
